{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\n\nclass Config:\n    AUTHOR = \"UnderDog\"\n\n    NAME = \"NFLC-\" + \"lgbm+xgb+stacking\"\n\n    COMPETITION = \"nfl-player-contact-detection\"\n\n    seed = 42\n    num_fold = 5\n    \n    lgbm_params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate':0.03,\n        'lambda_l1': 2.757654517864576e-06, \n        'lambda_l2': 0.018135558360332416, \n        'num_leaves': 254, \n        'feature_fraction': 0.9083150639158681, \n        'bagging_fraction': 0.7563425196831307, \n        'bagging_freq': 5, \n        'min_child_samples': 33\n    }\n    xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'learning_rate':0.03,\n        'verbosity': 2,\n        'booster': 'gbtree', \n        'lambda': 2.2456853514324213e-05, \n        'alpha': 0.010167492143658599, \n        'max_depth': 10, \n        'eta': 0.3119573893909086, \n        'gamma': 3.858135609025019e-05, \n        'colsample_bytree': 0.4158915520852815, \n        'colsample_bylevel': 0.605320595063157, \n        'subsample': 0.9302938053468902, \n        'min_child_weight': 0.1204339344286855,\n        'tree_method':'hist' if not torch.cuda.is_available() else 'gpu_hist'\n    }","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.922952,"end_time":"2023-02-28T04:11:49.378427","exception":false,"start_time":"2023-02-28T04:11:47.455475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:14.094415Z","iopub.execute_input":"2023-03-01T06:19:14.095127Z","iopub.status.idle":"2023-03-01T06:19:14.103277Z","shell.execute_reply.started":"2023-03-01T06:19:14.095090Z","shell.execute_reply":"2023-03-01T06:19:14.102087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport subprocess\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom IPython.display import Video, display\n\nfrom scipy.optimize import minimize\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import (\n    roc_auc_score,\n    matthews_corrcoef,\n)\n\nimport xgboost as xgb\nimport lightgbm as lgbm\n\nimport torch\n\nif torch.cuda.is_available():\n    import cupy \n    import cudf\n    from cuml import ForestInference","metadata":{"papermill":{"duration":4.622371,"end_time":"2023-02-28T04:11:54.005751","exception":false,"start_time":"2023-02-28T04:11:49.383380","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:18.191971Z","iopub.execute_input":"2023-03-01T06:19:18.192603Z","iopub.status.idle":"2023-03-01T06:19:18.199926Z","shell.execute_reply.started":"2023-03-01T06:19:18.192564Z","shell.execute_reply":"2023-03-01T06:19:18.198742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup(cfg):\n    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # set dirs\n    cfg.INPUT = f'../input/{cfg.COMPETITION}'\n    cfg.EXP = cfg.NAME\n    cfg.OUTPUT_EXP = cfg.NAME\n    cfg.SUBMISSION = './'\n    cfg.DATASET = '../input/'\n\n    cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n    cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n    cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n\n    # make dirs\n    for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n        \n    return cfg","metadata":{"papermill":{"duration":0.01532,"end_time":"2023-02-28T04:11:54.026417","exception":false,"start_time":"2023-02-28T04:11:54.011097","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:21.397294Z","iopub.execute_input":"2023-03-01T06:19:21.398005Z","iopub.status.idle":"2023-03-01T06:19:21.405279Z","shell.execute_reply.started":"2023-03-01T06:19:21.397967Z","shell.execute_reply":"2023-03-01T06:19:21.403855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==============================\n# function\n# ==============================\n# ref: https://www.kaggle.com/code/robikscube/nfl-player-contact-detection-getting-started\ndef add_contact_id(df):\n    # Create contact ids\n    df[\"contact_id\"] = (\n        df[\"game_play\"]\n        + \"_\"\n        + df[\"step\"].astype(\"str\")\n        + \"_\"\n        + df[\"nfl_player_id_1\"].astype(\"str\")\n        + \"_\"\n        + df[\"nfl_player_id_2\"].astype(\"str\")\n    )\n    return df\n\ndef expand_contact_id(df):\n    \"\"\"\n    Splits out contact_id into seperate columns.\n    \"\"\"\n    df[\"game_play\"] = df[\"contact_id\"].str[:12]\n    df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n    df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n    df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n    return df\n\n# cross validation\ndef get_groupkfold(train, target_col, group_col, n_splits):\n    kf = GroupKFold(n_splits=n_splits)\n    generator = kf.split(train, train[target_col], train[group_col])\n    fold_series = []\n    for fold, (idx_train, idx_valid) in enumerate(generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series\n\n# lgbm code\ndef fit_lgbm(cfg, X, y, params, add_suffix=''):\n\n    oof_pred = np.zeros(len(y), dtype=np.float32)\n    for fold in sorted(cfg.folds.unique()):\n        if fold == -1: continue\n        idx_train = (cfg.folds!=fold)\n        idx_valid = (cfg.folds==fold)\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        display(pd.Series(y_valid).value_counts())\n\n        lgbm_train = lgbm.Dataset(x_train, label=y_train)\n        lgbm_valid = lgbm.Dataset(x_valid, label=y_valid)\n        evals = [lgbm_train, lgbm_valid]\n\n        model = lgbm.train(\n            params,\n            lgbm_train,\n            num_boost_round=10_000,\n            early_stopping_rounds=100,\n            valid_sets=evals,\n            verbose_eval=100,\n        )\n\n        model_path = os.path.join(cfg.EXP_MODEL, f'lgbm_fold{fold}{add_suffix}.model')\n        model.save_model(model_path)\n        if not torch.cuda.is_available():\n            model = lgbm.Booster(model_file=model_path)\n        else:\n            model = ForestInference.load(model_path, output_class=True, model_type='lightgbm')\n        pred_i = model.predict(x_valid)\n        oof_pred[x_valid.index] = pred_i\n        score = round(roc_auc_score(y_valid, pred_i), 5)\n        print(f'Performance of the prediction: {score}\\n')\n        del model; gc.collect()\n\n    np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred{add_suffix}'), oof_pred)\n    score = round(roc_auc_score(y, oof_pred), 5)\n    print(f'All Performance of the prediction: {score}')\n    return oof_pred\n\ndef pred_lgbm(X, data_dir, add_suffix=''):\n    models = glob(os.path.join(data_dir, f'lgbm_fold*{add_suffix}.model'))\n    models = [lgbm.Booster(model_file=model_path) for model_path in models]\n    preds = np.array([model.predict(X) for model in models])\n    preds = np.mean(preds, axis=0)\n    return preds\n\n# xgboost code\ndef fit_xgboost(cfg, X, y, params, add_suffix=''):\n    \"\"\"\n    xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'learning_rate':0.01,\n        'tree_method':'gpu_hist'\n    }\n    \"\"\"\n    oof_pred = np.zeros(len(y), dtype=np.float32)\n    for fold in sorted(cfg.folds.unique()):\n        if fold == -1: continue\n        idx_train = (cfg.folds!=fold)\n        idx_valid = (cfg.folds==fold)\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        display(pd.Series(y_valid).value_counts())\n\n        xgb_train = xgb.DMatrix(x_train, label=y_train)\n        xgb_valid = xgb.DMatrix(x_valid, label=y_valid)\n        evals = [(xgb_train,'train'),(xgb_valid,'eval')]\n\n        model = xgb.train(\n            params,\n            xgb_train,\n            num_boost_round=10_000,\n            early_stopping_rounds=100,\n            evals=evals,\n            verbose_eval=100,\n        )\n\n        model_path = os.path.join(cfg.EXP_MODEL, f'xgb_fold{fold}{add_suffix}.model')\n        model.save_model(model_path)\n        if not torch.cuda.is_available():\n            model = xgb.Booster().load_model(model_path)\n        else:\n            model = ForestInference.load(model_path, output_class=True, model_type='xgboost')\n        pred_i = model.predict_proba(x_valid)[:, 1]\n        oof_pred[x_valid.index] = pred_i\n        score = round(roc_auc_score(y_valid, pred_i), 5)\n        print(f'Performance of the prediction: {score}\\n')\n        del model; gc.collect()\n\n    np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred{add_suffix}'), oof_pred)\n    score = round(roc_auc_score(y, oof_pred), 5)\n    print(f'All Performance of the prediction: {score}')\n    return oof_pred\n\ndef pred_xgboost(X, data_dir, add_suffix=''):\n    models = glob(os.path.join(data_dir, f'xgb_fold*{add_suffix}.model'))\n    if not torch.cuda.is_available():\n         models = [xgb.Booster().load_model(model_path) for model in models]\n    else:\n        models = [ForestInference.load(model, output_class=True, model_type='xgboost') for model in models]\n    preds = np.array([model.predict_proba(X)[:, 1] for model in models])\n    preds = np.mean(preds, axis=0)\n    return preds","metadata":{"papermill":{"duration":0.034509,"end_time":"2023-02-28T04:11:54.065798","exception":false,"start_time":"2023-02-28T04:11:54.031289","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:23.538977Z","iopub.execute_input":"2023-03-01T06:19:23.539400Z","iopub.status.idle":"2023-03-01T06:19:23.566321Z","shell.execute_reply.started":"2023-03-01T06:19:23.539365Z","shell.execute_reply":"2023-03-01T06:19:23.565303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==============================\n# read data\n# ==============================\ncfg = setup(Config)\n\nif not torch.cuda.is_available():\n    tr_tracking = pd.read_csv(os.path.join(cfg.INPUT, 'train_player_tracking.csv'), parse_dates=[\"datetime\"])\n    te_tracking = pd.read_csv(os.path.join(cfg.INPUT, 'test_player_tracking.csv'), parse_dates=[\"datetime\"])\n    # tr_helmets = pd.read_csv(os.path.join(cfg.INPUT, 'train_baseline_helmets.csv'))\n    # te_helmets = pd.read_csv(os.path.join(cfg.INPUT, 'test_baseline_helmets.csv'))\n    # tr_video_metadata = pd.read_csv(os.path.join(cfg.INPUT, 'train_video_metadata.csv'))\n    # te_video_metadata = pd.read_csv(os.path.join(cfg.INPUT, 'test_video_metadata.csv'))\n    sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n\n    train = pd.read_csv(os.path.join(cfg.INPUT, 'train_labels.csv'), parse_dates=[\"datetime\"])\n    test = expand_contact_id(sub)\n    \nelse:\n    tr_tracking = cudf.read_csv(os.path.join(cfg.INPUT, 'train_player_tracking.csv'), parse_dates=[\"datetime\"])\n    te_tracking = cudf.read_csv(os.path.join(cfg.INPUT, 'test_player_tracking.csv'), parse_dates=[\"datetime\"])\n    # tr_helmets = cudf.read_csv(os.path.join(cfg.INPUT, 'train_baseline_helmets.csv'))\n    # te_helmets = cudf.read_csv(os.path.join(cfg.INPUT, 'test_baseline_helmets.csv'))\n    # tr_video_metadata = cudf.read_csv(os.path.join(cfg.INPUT, 'train_video_metadata.csv'))\n    # te_video_metadata = cudf.read_csv(os.path.join(cfg.INPUT, 'test_video_metadata.csv'))\n    sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n\n    train = cudf.read_csv(os.path.join(cfg.INPUT, 'train_labels.csv'), parse_dates=[\"datetime\"])\n    test = cudf.DataFrame(expand_contact_id(sub))","metadata":{"papermill":{"duration":13.864952,"end_time":"2023-02-28T04:12:07.935643","exception":false,"start_time":"2023-02-28T04:11:54.070691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:31.270280Z","iopub.execute_input":"2023-03-01T06:19:31.271444Z","iopub.status.idle":"2023-03-01T06:19:35.726197Z","shell.execute_reply.started":"2023-03-01T06:19:31.271396Z","shell.execute_reply":"2023-03-01T06:19:35.725023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code is used to create the features.  \nBasically, the numerical features contained in player_tracking.csv are merged into player_id_1 and player_id_2 respectively.","metadata":{"papermill":{"duration":0.004608,"end_time":"2023-02-28T04:12:07.945607","exception":false,"start_time":"2023-02-28T04:12:07.940999","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ==============================\n# feature engineering\n# ==============================\ndef create_features(df, tr_tracking, merge_col=\"step\", use_cols=[\"x_position\", \"y_position\"]):\n    output_cols = []\n    df_combo = (\n        df.astype({\"nfl_player_id_1\": \"str\"})\n        .merge(\n            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n                [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n            ],\n            left_on=[\"game_play\", merge_col, \"nfl_player_id_1\"],\n            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n            how=\"left\",\n        )\n        .rename(columns={c: c+\"_1\" for c in use_cols})\n        .drop(\"nfl_player_id\", axis=1)\n        .merge(\n            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n                [\"game_play\", merge_col, \"nfl_player_id\"] + use_cols\n            ],\n            left_on=[\"game_play\", merge_col, \"nfl_player_id_2\"],\n            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n            how=\"left\",\n        )\n        .drop(\"nfl_player_id\", axis=1)\n        .rename(columns={c: c+\"_2\" for c in use_cols})\n        .sort_values([\"game_play\", merge_col, \"nfl_player_id_1\", \"nfl_player_id_2\"])\n        .reset_index(drop=True)\n    )\n    output_cols += [c+\"_1\" for c in use_cols]\n    output_cols += [c+\"_2\" for c in use_cols]\n    \n    if (\"x_position\" in use_cols) & (\"y_position\" in use_cols):\n        index = df_combo['x_position_2'].notnull()\n        if torch.cuda.is_available():\n            index = index.to_array()\n        distance_arr = np.full(len(index), np.nan)\n        tmp_distance_arr = np.sqrt(\n            np.square(df_combo.loc[index, \"x_position_1\"] - df_combo.loc[index, \"x_position_2\"])\n            + np.square(df_combo.loc[index, \"y_position_1\"]- df_combo.loc[index, \"y_position_2\"])\n        )\n        if torch.cuda.is_available():\n            tmp_distance_arr = tmp_distance_arr.to_array()\n        distance_arr[index] = tmp_distance_arr\n        df_combo['distance'] = distance_arr\n        output_cols += [\"distance\"]\n        \n    df_combo['G_flug'] = (df_combo['nfl_player_id_2']==\"G\")\n    output_cols += [\"G_flug\"]\n    return df_combo, output_cols\n\n\nuse_cols = [\n    'x_position', 'y_position', 'speed', 'distance',\n    'direction', 'orientation', 'acceleration', 'sa'\n]\ntrain, feature_cols = create_features(train, tr_tracking, use_cols=use_cols)\ntest, feature_cols = create_features(test, te_tracking, use_cols=use_cols)\nif torch.cuda.is_available():\n    train = train.to_pandas()\n    test = test.to_pandas()\n\ndisplay(train)","metadata":{"papermill":{"duration":7.675643,"end_time":"2023-02-28T04:12:15.626383","exception":false,"start_time":"2023-02-28T04:12:07.950740","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:37.277582Z","iopub.execute_input":"2023-03-01T06:19:37.278111Z","iopub.status.idle":"2023-03-01T06:19:44.224331Z","shell.execute_reply.started":"2023-03-01T06:19:37.278076Z","shell.execute_reply":"2023-03-01T06:19:44.223226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exclude distance > 2\nif the distance between two players is greater than 2 then the probability of contact is so low, we will consider it = 0, training data will be reduced from 4.7 M rows to 660 K","metadata":{"papermill":{"duration":0.005074,"end_time":"2023-02-28T04:12:15.637303","exception":false,"start_time":"2023-02-28T04:12:15.632229","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DISTANCE_THRESH = 2\n\ntrain_y = train['contact'].values\noof_pred = np.zeros(len(train))\ncond_dis_train = (train['distance']<=DISTANCE_THRESH) | (train['distance'].isna())\ncond_dis_test = (test['distance']<=DISTANCE_THRESH) | (test['distance'].isna())\n\ntrain = train[cond_dis_train]\ntrain.reset_index(inplace = True, drop = True)\n\nprint('number of train data : ',len(train))\n\n_ = gc.collect()","metadata":{"papermill":{"duration":0.438698,"end_time":"2023-02-28T04:12:16.081295","exception":false,"start_time":"2023-02-28T04:12:15.642597","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:44.599050Z","iopub.execute_input":"2023-03-01T06:19:44.600192Z","iopub.status.idle":"2023-03-01T06:19:45.109102Z","shell.execute_reply.started":"2023-03-01T06:19:44.600148Z","shell.execute_reply":"2023-03-01T06:19:45.107838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helmet track Features","metadata":{"papermill":{"duration":0.005279,"end_time":"2023-02-28T04:12:16.091871","exception":false,"start_time":"2023-02-28T04:12:16.086592","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CLUSTERS = [10, 50, 100, 500]\n\ndef add_step_pct(df, cluster):\n    df['step_pct'] = cluster * (df['step']-min(df['step']))/(max(df['step'])-min(df['step']))\n    df['step_pct'] = df['step_pct'].apply(np.ceil).astype(np.int32)\n    return df\n\nfor cluster in CLUSTERS:\n    train = train.groupby('game_play').apply(lambda x:add_step_pct(x,cluster))\n    test = test.groupby('game_play').apply(lambda x:add_step_pct(x,cluster))\n\n    for helmet_view in ['Sideline', 'Endzone']:\n        helmet_train = pd.read_csv('/kaggle/input/nfl-player-contact-detection/train_baseline_helmets.csv')\n        helmet_train.loc[helmet_train['view']=='Endzone2','view'] = 'Endzone'\n        helmet_test = pd.read_csv('/kaggle/input/nfl-player-contact-detection/test_baseline_helmets.csv')\n        helmet_test.loc[helmet_test['view']=='Endzone2','view'] = 'Endzone'\n\n        helmet_train.rename(columns = {'frame': 'step'}, inplace = True)\n        helmet_train = helmet_train.groupby('game_play').apply(lambda x:add_step_pct(x,cluster))\n        helmet_test.rename(columns = {'frame': 'step'}, inplace = True)\n        helmet_test = helmet_test.groupby('game_play').apply(lambda x:add_step_pct(x,cluster))\n        helmet_train = helmet_train[helmet_train['view']==helmet_view]\n        helmet_test = helmet_test[helmet_test['view']==helmet_view]\n\n        helmet_train['helmet_id'] = helmet_train['game_play'] + '_' + helmet_train['nfl_player_id'].astype(str) + '_' + helmet_train['step_pct'].astype(str)\n        helmet_test['helmet_id'] = helmet_test['game_play'] + '_' + helmet_test['nfl_player_id'].astype(str) + '_' + helmet_test['step_pct'].astype(str)\n\n        helmet_train = helmet_train[['helmet_id', 'left', 'width', 'top', 'height']].groupby('helmet_id').mean().reset_index()\n        helmet_test = helmet_test[['helmet_id', 'left', 'width', 'top', 'height']].groupby('helmet_id').mean().reset_index()\n        for player_ind in [1, 2]:\n            train['helmet_id'] = train['game_play'] + '_' + train['nfl_player_id_'+str(player_ind)].astype(str) + \\\n                                    '_' + train['step_pct'].astype(str)\n            test['helmet_id'] = test['game_play'] + '_' + test['nfl_player_id_'+str(player_ind)].astype(str) + \\\n                                    '_' + test['step_pct'].astype(str)\n\n            train = train.merge(helmet_train, how = 'left')\n            test = test.merge(helmet_test, how = 'left')\n\n            train.rename(columns = {i:i+'_'+helmet_view+'_'+str(cluster)+'_'+str(player_ind) for i in ['left', 'width', 'top', 'height']}, inplace = True)\n            test.rename(columns = {i:i+'_'+helmet_view+'_'+str(cluster)+'_'+str(player_ind) for i in ['left', 'width', 'top', 'height']}, inplace = True)\n\n            del train['helmet_id'], test['helmet_id']\n            gc.collect()\n\n            feature_cols += [i+'_'+helmet_view+'_'+str(cluster)+'_'+str(player_ind) for i in ['left', 'width', 'top', 'height']]\n        del helmet_train, helmet_test\n        gc.collect()","metadata":{"papermill":{"duration":146.579475,"end_time":"2023-02-28T04:14:42.676409","exception":false,"start_time":"2023-02-28T04:12:16.096934","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:19:49.580772Z","iopub.execute_input":"2023-03-01T06:19:49.581143Z","iopub.status.idle":"2023-03-01T06:22:15.606907Z","shell.execute_reply.started":"2023-03-01T06:19:49.581111Z","shell.execute_reply":"2023-03-01T06:22:15.605822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fill missing values for the ground","metadata":{"papermill":{"duration":0.005808,"end_time":"2023-02-28T04:14:42.688623","exception":false,"start_time":"2023-02-28T04:14:42.682815","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 이 코드는 충돌 판정 시, 양쪽 선수의 위치 정보를 비슷하게 만들어주는 역할을 합니다.\nfor cluster in CLUSTERS:\n    for helmet_view in ['Sideline', 'Endzone']:\n        train.loc[train['G_flug']==True,'left_'+helmet_view+'_'+str(cluster)+'_2'] = train.loc[train['G_flug']==True,'left_'+helmet_view+'_'+str(cluster)+'_1']\n        train.loc[train['G_flug']==True,'top_'+helmet_view+'_'+str(cluster)+'_2'] = train.loc[train['G_flug']==True,'top_'+helmet_view+'_'+str(cluster)+'_1']\n        train.loc[train['G_flug']==True,'width_'+helmet_view+'_'+str(cluster)+'_2'] = 0\n        train.loc[train['G_flug']==True,'height_'+helmet_view+'_'+str(cluster)+'_2'] = 0\n        \n        test.loc[test['G_flug']==True,'left_'+helmet_view+'_'+str(cluster)+'_2'] = test.loc[test['G_flug']==True,'left_'+helmet_view+'_'+str(cluster)+'_1']\n        test.loc[test['G_flug']==True,'top_'+helmet_view+'_'+str(cluster)+'_2'] = test.loc[test['G_flug']==True,'top_'+helmet_view+'_'+str(cluster)+'_1']\n        test.loc[test['G_flug']==True,'width_'+helmet_view+'_'+str(cluster)+'_2'] = 0\n        test.loc[test['G_flug']==True,'height_'+helmet_view+'_'+str(cluster)+'_2'] = 0","metadata":{"papermill":{"duration":0.435114,"end_time":"2023-02-28T04:14:43.129588","exception":false,"start_time":"2023-02-28T04:14:42.694474","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:22:15.609065Z","iopub.execute_input":"2023-03-01T06:22:15.610759Z","iopub.status.idle":"2023-03-01T06:22:16.003774Z","shell.execute_reply.started":"2023-03-01T06:22:15.610717Z","shell.execute_reply":"2023-03-01T06:22:16.002618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Diffrence & Product features","metadata":{"papermill":{"duration":0.005159,"end_time":"2023-02-28T04:14:43.140574","exception":false,"start_time":"2023-02-28T04:14:43.135415","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cols = [i[:-2] for i in train.columns if i[-2:]=='_1' and i!='nfl_player_id_1']\ntrain[[i+'_diff' for i in cols]] = np.abs(train[[i+'_1' for i in cols]].values - train[[i+'_2' for i in cols]].values)\ntest[[i+'_diff' for i in cols]] = np.abs(test[[i+'_1' for i in cols]].values - test[[i+'_2' for i in cols]].values)\nfeature_cols += [i+'_diff' for i in cols]\n\ncols = ['x_position', 'y_position', 'speed', 'distance', 'direction', 'orientation', 'acceleration', 'sa']\ntrain[[i+'_prod' for i in cols]] = train[[i+'_1' for i in cols]].values * train[[i+'_2' for i in cols]].values\ntest[[i+'_prod' for i in cols]] = test[[i+'_1' for i in cols]].values * test[[i+'_2' for i in cols]].values\nfeature_cols += [i+'_prod' for i in cols]\n\nprint('number of features : ',len(feature_cols))\nprint('number of train data : ',len(train))","metadata":{"papermill":{"duration":1.186843,"end_time":"2023-02-28T04:14:44.332721","exception":false,"start_time":"2023-02-28T04:14:43.145878","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:22:16.005555Z","iopub.execute_input":"2023-03-01T06:22:16.005954Z","iopub.status.idle":"2023-03-01T06:22:17.317263Z","shell.execute_reply.started":"2023-03-01T06:22:16.005912Z","shell.execute_reply":"2023-03-01T06:22:17.316139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Infer XGBoost model","metadata":{"papermill":{"duration":0.00552,"end_time":"2023-02-28T04:14:44.344191","exception":false,"start_time":"2023-02-28T04:14:44.338671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ==============================\n# training & inference\n# ==============================\n\ncfg.folds = get_groupkfold(train, 'contact', 'game_play', cfg.num_fold)\ncfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'), index=False)\n\noof_pred[np.where(cond_dis_train)] = fit_lgbm(cfg, train[feature_cols], train['contact'], \n                                              cfg.lgbm_params, add_suffix=\"_lgbm_1st\")\nnp.save('oof_pred_1.npy',oof_pred)\nsub_pred_1 = pred_lgbm(test.loc[cond_dis_test, feature_cols], cfg.EXP_MODEL, add_suffix=\"_lgbm_1st\")","metadata":{"papermill":{"duration":2553.928235,"end_time":"2023-02-28T04:57:18.277745","exception":false,"start_time":"2023-02-28T04:14:44.349510","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T06:22:17.320204Z","iopub.execute_input":"2023-03-01T06:22:17.320974Z","iopub.status.idle":"2023-03-01T07:03:37.746015Z","shell.execute_reply.started":"2023-03-01T06:22:17.320936Z","shell.execute_reply":"2023-03-01T07:03:37.744762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def func(x_list):\n    score = matthews_corrcoef(train_y, oof_pred>x_list[0])\n    return -score\n\nx0 = [0.5]\nresult = minimize(func, x0,  method=\"nelder-mead\")\ncfg.threshold = result.x[0]\nprint(\"score:\", round(matthews_corrcoef(train_y, oof_pred>cfg.threshold), 5))\nprint(\"threshold\", round(cfg.threshold, 5))\n\nsub_pred_1 = (sub_pred_1 > cfg.threshold).astype(int)\n# sub_pred_1이 6631개의 0,1 값으로 변경되는지 확인\nsub_pred_1","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:03:37.747910Z","iopub.execute_input":"2023-03-01T07:03:37.748657Z","iopub.status.idle":"2023-03-01T07:04:11.077547Z","shell.execute_reply.started":"2023-03-01T07:03:37.748614Z","shell.execute_reply":"2023-03-01T07:04:11.076447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.folds = get_groupkfold(train, 'contact', 'game_play', cfg.num_fold)\ncfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'), index=False)\n\noof_pred[np.where(cond_dis_train)] = fit_xgboost(cfg, train[feature_cols], train['contact'], \n                                              cfg.xgb_params, add_suffix=\"_xgb_1st\")\nnp.save('oof_pred_2.npy',oof_pred)\nsub_pred_2 = pred_xgboost(test.loc[cond_dis_test, feature_cols], cfg.EXP_MODEL, add_suffix=\"_xgb_1st\")","metadata":{"papermill":{"duration":202.427639,"end_time":"2023-02-28T05:00:40.714586","exception":false,"start_time":"2023-02-28T04:57:18.286947","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T07:04:44.826759Z","iopub.execute_input":"2023-03-01T07:04:44.827721Z","iopub.status.idle":"2023-03-01T07:08:10.345847Z","shell.execute_reply.started":"2023-03-01T07:04:44.827661Z","shell.execute_reply":"2023-03-01T07:08:10.344529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x0 = [0.5]\nresult = minimize(func, x0,  method=\"nelder-mead\")\ncfg.threshold = result.x[0]\nprint(\"score:\", round(matthews_corrcoef(train_y, oof_pred>cfg.threshold), 5))   \nprint(\"threshold\", round(cfg.threshold, 5))\n\nsub_pred_2 = (sub_pred_2 > cfg.threshold).astype(int)\n# sub_pred_1이 6631개의 0,1 값으로 변경되는지 확인\nsub_pred_2","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:08:10.348197Z","iopub.execute_input":"2023-03-01T07:08:10.348603Z","iopub.status.idle":"2023-03-01T07:08:52.614315Z","shell.execute_reply.started":"2023-03-01T07:08:10.348561Z","shell.execute_reply":"2023-03-01T07:08:52.613245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sub_pred_1), len(sub_pred_2)","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:10:03.298375Z","iopub.execute_input":"2023-03-01T07:10:03.299067Z","iopub.status.idle":"2023-03-01T07:10:03.305791Z","shell.execute_reply.started":"2023-03-01T07:10:03.299031Z","shell.execute_reply":"2023-03-01T07:10:03.304736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.fillna(0)\n\nfrom sklearn.ensemble import  StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport lightgbm as lgb  \nimport xgboost as xgb\nimport catboost as cb\nfrom catboost import CatBoostClassifier\n# from sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 로드\ndata, target = train[feature_cols], train['contact']\ntrain_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n\nscore = 0\nmodels = []\n\n\nbase_models = [('rf_1', lgb.LGBMClassifier(\n                            objective = 'binary',\n                            learning_rate = 0.03, \n                            lambda_l1 = 2.757654517864576e-06,\n#                             lambda_l2 = 0.018135558360332416,\n                            num_leaves = 254,\n                            feature_fraction = 0.9083150639158681,\n                            bagging_fraction = 0.7563425196831307,\n                            bagging_freq = 5,\n                            min_child_samples = 33\n                     )),\n               \n               ('rf_2', cb.CatBoostClassifier(\n                            loss_function = 'Logloss',\n                            eval_metric = 'Accuracy',\n                            verbose = False,\n                            depth = 9,\n                            learning_rate = 0.09349732050796461,\n                            l2_leaf_reg = 0.0026112334190675508, \n                            bagging_temperature = 0.004346654343182356,\n                            random_strength = 0.0002090724949890479, \n                            border_count = 99\n                     ))]\n\n    # stacking 설정\nmodel = StackingClassifier(estimators=base_models, final_estimator=xgb.XGBClassifier(max_depth=3,\n                                                                                     objective = 'binary:logistic',\n                                                                                     learning_rate= 0.03,\n                                                                                     n_estimators= 50,\n                                                                                     reg_alpha = 0.5,\n#                                        nthread = -1,\n                                                                                     min_child_weight=3,\n                                                                                     gamma=0.5,\n                                                                                     subsample=0.5\n#                                                                                      colsample_bytree=0.5\n                                                                                    ))\n\nmodel.fit(train_x, train_y)\nmodels.append(model)\n\npred = model.predict(valid_x)\npred_i = model.predict_proba(valid_x)[:, 1]\n\nscore = roc_auc_score(valid_y,(pred > 0.3).astype(int))","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:13:06.075319Z","iopub.execute_input":"2023-03-01T07:13:06.075709Z","iopub.status.idle":"2023-03-01T07:51:01.563243Z","shell.execute_reply.started":"2023-03-01T07:13:06.075669Z","shell.execute_reply":"2023-03-01T07:51:01.562071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.array([model.predict_proba(test.loc[cond_dis_test, feature_cols])[:, 1] for model in models])\npreds = np.mean(preds, axis=0)\npreds = (preds > cfg.threshold).astype(int)\npreds","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:54:56.788993Z","iopub.execute_input":"2023-03-01T07:54:56.789979Z","iopub.status.idle":"2023-03-01T07:54:56.962594Z","shell.execute_reply.started":"2023-03-01T07:54:56.789924Z","shell.execute_reply":"2023-03-01T07:54:56.961456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측값 배열 생성\npredictions = np.array([sub_pred_1, sub_pred_2, preds])\n\n# 하드보팅 적용하여 예측\nhard_vote = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)\n\n","metadata":{"papermill":{"duration":0.023558,"end_time":"2023-02-28T05:00:40.750853","exception":false,"start_time":"2023-02-28T05:00:40.727295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T07:55:19.718701Z","iopub.execute_input":"2023-03-01T07:55:19.719315Z","iopub.status.idle":"2023-03-01T07:55:19.772029Z","shell.execute_reply.started":"2023-03-01T07:55:19.719269Z","shell.execute_reply":"2023-03-01T07:55:19.771030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hard_vote","metadata":{"execution":{"iopub.status.busy":"2023-03-01T07:55:50.428945Z","iopub.execute_input":"2023-03-01T07:55:50.429319Z","iopub.status.idle":"2023-03-01T07:55:50.436934Z","shell.execute_reply.started":"2023-03-01T07:55:50.429284Z","shell.execute_reply":"2023-03-01T07:55:50.435742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.010773,"end_time":"2023-02-28T05:00:40.773574","exception":false,"start_time":"2023-02-28T05:00:40.762801","status":"completed"},"tags":[]}},{"cell_type":"code","source":"del train\ngc.collect()\n\ntest = add_contact_id(test)\ntest['contact'] = 0\ntest.loc[cond_dis_test, 'contact'] = hard_vote\ntest[['contact_id', 'contact']].to_csv('submission.csv', index=False)\ndisplay(test[['contact_id', 'contact']].head())","metadata":{"papermill":{"duration":0.13003,"end_time":"2023-02-28T05:00:40.914517","exception":false,"start_time":"2023-02-28T05:00:40.784487","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-01T07:57:02.542058Z","iopub.execute_input":"2023-03-01T07:57:02.542815Z","iopub.status.idle":"2023-03-01T07:57:02.923917Z","shell.execute_reply.started":"2023-03-01T07:57:02.542773Z","shell.execute_reply":"2023-03-01T07:57:02.922720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.016795,"end_time":"2023-02-28T05:00:40.942646","exception":false,"start_time":"2023-02-28T05:00:40.925851","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}